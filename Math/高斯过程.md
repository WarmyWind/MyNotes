# 高斯过程

本篇内容主要来自Bishop的《Pattern Recognition and Machine Learning》6.4节。

## 回顾线性回归

我们从线性回归引出高斯过程。考虑一个$M$个基函数的线性组合模型：

$$
y\left( {\bf{x}} \right) = {{\bf{w}}^T}\bm\phi \left( {\bf{x}} \right)
$$

其中$\bf x$是输入向量，$\bf w$是$M$维权重向量，$\bm \phi \left( {\bf{x}} \right){\rm{ = }}\left( {{\phi _0}\left( {\bf{x}} \right), \cdots ,{\phi _{M - 1}}\left( {\bf{x}} \right)} \right)^T$，并设$\bf w$的先验分布为$p\left( {\bf{w}} \right) = \mathcal N\left( {{\bf{w}}\left| {{\bf{0}},{\alpha ^{ - 1}}{\bf{I}}} \right.} \right)$。

观察到训练点${\bf x}_1,\cdots,{\bf x}_N$，我们想知道模型输出$y_n=y\left({\bf x}_n\right),n=1,\cdots,N$的联合分布。从模型可以知道

$$
{\bf{y}} = {\bf{\Phi w}}
$$

其中$\bf \Phi$的元素$\Phi_{nk}=\phi_k\left({\bf x}_n\right)$。由于权重是高斯的，显然输出的联合分布也是高斯的，其均值和协方差为

$$
\mathbb E\left[ {\bf{y}} \right] = {\bf{\Phi }}\mathbb E\left[ {\bf{w}} \right] = {\bf{0}}
$$

$$
{{\rm cov}} \left[ {\bf{y}} \right] = E\left[ {{\bf{y}}{{\bf{y}}^T}} \right] = {\bf{\Phi }}E\left[ {{\bf{w}}{{\bf{w}}^T}} \right]{{\bf{\Phi }}^T} = {\alpha ^{ - 1}}{\bf{\Phi }}{{\bf{\Phi }}^T=\bf K}
$$

$\bf K$是Gram矩阵，其元素$K_{nm}=k\left({\bf x}_n,{\bf x}_m\right)=\alpha^{-1}{\bm \phi\left({\bf x}_n\right)}^T \bm\phi\left({\bf x}_m\right)$，$k$是核函数。从$\rm cov\left[\bf y\right]$的计算式可以看出核函数满足

$$
k\left( {{{\bf{x}}_n},{{\bf{x}}_m}} \right) = E\left[ {y\left( {{{\bf{x}}_n}} \right)y\left( {{{\bf{x}}_m}} \right)} \right]
$$

因此我们可以直接定义核函数来得到输出协方差阵的形式，而不用选择具体的基函数。

## 高斯过程回归

我们对线性模型加入噪声来得到目标$t_n$

$$
{t_n} = {y_n} + {\epsilon _n}
$$

假设噪声$\epsilon_n$是独立同分布的，给定$y_n=y\left({\bf x}_n\right),n=1,\cdots,N$，$t_n$的联合概率为

$$
p\left( {{\bf{t}}\left| {\bf{y}} \right.} \right) = \mathcal N\left( {{\bf{t}}\left| {{\bf{y}},{\beta ^{ - 1}}{{\bf{I}}_n}} \right.} \right)
$$

而$y_n$的联合概率为

$$
p\left( {\bf{y}} \right) = \mathcal N\left( {{\bf{y}}\left| {{\bf{0}},{\bf{K}}} \right.} \right)
$$

那么$t_n$的联合边缘概率分布为

$$
p\left( {\bf{t}} \right) = \int {p\left( {{\bf{t}}\left| {\bf{y}} \right.} \right)p\left( {\bf{y}} \right)d{\bf{y}} = \mathcal N\left( {{\bf{t}}\left| {{\bf{0}},{\bf{C}}} \right.} \right)}
$$

其中

$$
{\bf{C}} = {\bf{K}} + {\beta ^{ - 1}}{{\bf{I}}_n}
$$

实际上${\bf{t}} = {\bf{y}} + {\bm{\epsilon }}$，$\bf{y}$与${\bm{\epsilon }}$是独立的，因此${\bf{t}}$的协方差为两者的协方差直接相加。

之后我们对新的观测数据${\bf x}^*=\left({\bf x}_{n+1},\cdots,{\bf x}_{n+m+1}\right)^T$预测${\bf t}^*=\left({\bf t}_{n+1},\cdots,{\bf t}_{n+m+1}\right)^T$，假设新的目标加入后的联合分布依然服从高斯分布：

$$
\left[ {\begin{array}{ccccccccccccccc}{\bf{y}}\\{{{\bf{y}}^*}}\end{array}} \right] \sim \mathcal N\left( {{\bf{0}},\left[ {\begin{array}{ccccccccccccccc}{{\bf{C}}\left( {{\bf{x}},{\bf{x}}} \right)}&{{\bf{K}}\left( {{\bf{x}},{{\bf{x}}^*}} \right)}\\{{\bf{K}}\left( {{{\bf{x}}^*},{\bf{x}}} \right)}&{{\bf{C}}\left( {{{\bf{x}}^*},{{\bf{x}}^*}} \right)}\end{array}} \right]} \right)
$$

> 多维高斯分布的条件概率如下：
>
> 若
>
> $$
> \left[ {\begin{array}{ccccccccccccccc}{\bf{x}}\\{\bf{y}}\end{array}} \right] \sim N\left( {\left[ {\begin{array}{ccccccccccccccc}{{{\bm{\mu }}_x}}\\{{{\bm{\mu }}_y}}\end{array}} \right],\left[ {\begin{array}{ccccccccccccccc}{\bf{A}}&{\bf{C}}\\{{{\bf{C}}^T}}&{\bf{B}}\end{array}} \right]} \right)
> $$
>
> 则
>
> $$
> {\bf{x}}\left| {{\bf{y}} \sim N\left( {{{\bm{\mu }}_x} + {\bf{C}}{{\bf{B}}^{ - 1}}\left( {{\bf{y}} - {{\bm{\mu }}_y}} \right),{\bf{A}} - {\bf{C}}{{\bf{B}}^{ - 1}}{{\bf{C}}^T}} \right)} \right.
> $$

根据多维高斯的条件分布公式，可得

$$
\mathbb E\left[ {{{\bf{y}}^*}\left| {\bf{y}} \right.} \right] = {\bf{K}}\left( {{{\bf{x}}^*},{\bf{x}}} \right){\bf{C}}{\left( {{\bf{x}},{\bf{x}}} \right)^{ - 1}}{\bf{y}}
$$

$$
{\bf{C}}\left[ {{{\bf{y}}^*}\left| {\bf{y}} \right.} \right] = {\bf{C}}\left( {{{\bf{x}}^*},{{\bf{x}}^*}} \right) - {\bf{K}}\left( {{{\bf{x}}^*},{\bf{x}}} \right){\bf{K}}{\left( {{\bf{x}},{\bf{x}}} \right)^{ - 1}}{\bf{K}}\left( {{\bf{x}},{{\bf{x}}^*}} \right)
$$

这就得到了预测值的均值和协方差。
