# Layer-Wise Relevance Propagation: An Overview

LRP是一种解释神经网络的方法。它通过将预测结果在神经网络中反向传播来实现的，其中的传播规则是经过设计的。本文就对LRP进行介绍和讨论。

## Introduction

如今的机器学习方法越来越依赖于大规模的数据集，但是大型数据集经常受到不同变量之间存在的虚假相关性的困扰。当模型需要选择一些相关的输入变量来预测结果时，输入变量之间的虚假相关性就会造成干扰。如果模型没有从训练集中学习到正确的数据特征，那么预测器的效果就会很差。

可解释机器学习采用的做法是：对模型的训练不需要那么关注特征选择，只有在训练完成后我们再观察神经网络学到了输入的哪些特征。基于这种解释性的反馈，我们可以在移除那些”差的”特征后重新训练。

> 我的理解：如果一些输入特征没有被模型学习，很可能是因为这些特征无用或者不具有代表性，因此比较“差”。而输入特征是否被学习，可以通过输入特征与预测结果的相关性来衡量。

一种简单的解释方法是泰勒分解，通过对预测$f(\bf x)$在附近的某个参考点${\bf{\tilde x}}$展开：

$$
f\left( {\bf{x}} \right) = f\left( {{\bf{\tilde x}}} \right) + \sum\nolimits_{i = 1}^d {\left( {{x_i} - {{\tilde x}_i}} \right)}  \cdot {\left[ {\nabla f\left( {{\bf{\tilde x}}} \right)} \right]_i} +  \cdots
$$

一阶项量化了每个输入特征与预测结果之间的相关性，这形成了一种解释。

> 对此我的理解是：如果我们能选择一个合适的 ${\bf{\tilde x}}$，那么预测结果对于输入特征${\tilde x_i}$的偏导数就是${\left[ {\nabla f\left( {{\bf{\tilde x}}} \right)} \right]_i}$，这就反映了${\tilde x_i}$对输出结果的“贡献”，贡献越大相关性也就越大。

但这种方法在应用于深度神经网络时十分不稳定，主要是由于很难选择一个合适的参考点${\bf{\tilde x}}$。具体地来说，$f(\bf x)$的梯度噪声很大（不准），并且对输入的微小扰动都可能使$f(\bf x)$剧烈变化（不稳）。

下面就开始介绍LRP。

## Layer-Wise Relevance Propagation

让$j$和$k$表示在相邻网络层中的两个神经元，传播相关性分数（propagating relevance score）的计算式形式为

$$
{R_j} = \sum\limits_k {\frac{{{z_{jk}}}}{{\sum\nolimits_j {{z_{jk}}} }}{R_k}}
$$

其中${z_{jk}}$表示神经元$j$对神经元$k$的相关性的贡献程度，${z_{jk}}$可以定义不同的计算方式。

> 文中对该公式的解释不是很详细。我的理解是，该公式的
>
> $$
> {\frac{{{z_{jk}}}}{{\sum\nolimits_j {{z_{jk}}} }}{R_k}}
> $$
>
> 实际上就是神经元$j$对下一层网络中的**某个神经元**$k$的相关性贡献，归一系数代表了贡献比例。
>
> 而$R_j$作为求和，实际上就是神经元$j$对**下一层网络**总的相关性贡献。如果这个贡献很小，那就说明神经元$j$不是很相关。

### 应用ReLU的深度神经网络的LRP规则

下面我们考虑在应用ReLU作为激活函数的神经网络中的LRP规则（所谓不同规则，就是通过$R_j$的不同具体计算方式来区分）。

#### LRP-0

一种直接的想法是通过值的贡献来定义相关性贡献：

$$
{R_j} = \sum\limits_k {\frac{{{a_j}{w_{jk}}}}{{\sum\nolimits_{0,j} {{a_j}{w_{jk}}} }}{R_k}}
$$

其中$a_j$表示神经元$j$的值，$w_{jk}$表示神经元$j$到神经元$k$之间的权重，并且定义$a_0=1$和$w_{0k}$表示神经元$k$的偏置。

这种方法实际上就是将输入的相关性解释成输入$\times$梯度，其等价于敏感度分析（即梯度解释方法）。

#### LRP-$\epsilon$

在LRP-0的基础上修改为

$$
{R_j} = \sum\limits_k {\frac{{{a_j}{w_{jk}}}}{{\varepsilon  + \sum\nolimits_{0,j} {{a_j}{w_{jk}}} }}{R_k}}
$$

当$\epsilon$变大，只有显著的特征能够留下来，这将导致最终的相关性解释是稀疏的。这种方法相较于LRP-0可以更好地对抗梯度噪声。

#### LRP-$\gamma$

另一种增强方法为

$$
{R_j} = \sum\limits_k {\frac{{{a_j}\left( {{w_{jk}} + \gamma w_{jk}^ + } \right)}}{{\sum\nolimits_{0,j} {{a_j}\left( {{w_{jk}} + \gamma w_{jk}^ + } \right)} }}{R_k}}
$$

其中$w_{jk}^ +  = \max \left( {0,{w_{jk}}} \right)$，$\gamma$表示偏向于正贡献的程度。这种方法能给出更稳定的解释。
